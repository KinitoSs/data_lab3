{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyPkb4ho0gll9AyeCBgZxCCx"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo1HO5PCPwV2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Лабораторная работа №3. Web-scraping"
   ],
   "metadata": {
    "id": "9XbTDrbmQDHh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Часть 1. Парсинг сайта через библиотеки requests и Beautifulsoup и lxml"
   ],
   "metadata": {
    "id": "CIBn-8rvQJFR",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.imdb.com/search/title/?groups=top_1000&count=250'\n",
    "r = requests.get(url)\n",
    "r.content"
   ],
   "metadata": {
    "id": "1lJKq5O1TTgA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "s=str(r.text.encode('utf-8'))\n",
    "with open('test.html', 'w') as output_file:\n",
    "  output_file.write(s)"
   ],
   "metadata": {
    "id": "Ymz1DLxfTUk-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "header= {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "'accept-encoding': 'gzip, deflate, br',\n",
    "'accept-language': 'en,en-US;q=0.9',\n",
    "'cache-control': 'max-age=0',\n",
    "'referer': 'https://www.imdb.com/search/title/',\n",
    "'upgrade-insecure-requests': '1',\n",
    "'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36'\n",
    "}"
   ],
   "metadata": {
    "id": "IUFDg2_cTYeg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "url = 'https://www.imdb.com/search/title/?groups=top_1000&count=250'\n",
    "r = requests.get(url, headers=header, timeout=(3.05, 27))\n",
    "r.content"
   ],
   "metadata": {
    "id": "plj5oZbKTaws",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('test2.html', 'w') as output_file:\n",
    "  output_file.write(str(r.text.encode('utf-8')))"
   ],
   "metadata": {
    "id": "XxeykskMTdHF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#XPath — it is a language for querying xml and xhtml documents.\n",
    "from lxml import html\n",
    "test = '''\n",
    "    <html>\n",
    "        <body>\n",
    "            <div class=\"first_level\">\n",
    "                <h2 align='center'>one</h2>\n",
    "                <h2 align='left'>two</h2>\n",
    "            </div>\n",
    "            <h2>another tag</h2>\n",
    "        </body>\n",
    "    </html>\n",
    "'''\n",
    "tree = html.fromstring(test)\n",
    "tree.xpath('//h2') # все h2 теги\n",
    "tree.xpath('//h2[@align]') # h2 теги с атрибутом align\n",
    "tree.xpath('//h2[@align=\"center\"]') # h2 теги с атрибутом align равным \"center\"\n",
    "\n",
    "div_node = tree.xpath('//div')[0] # div тег\n",
    "div_node.xpath('.//h2') # все h2 теги, которые являются дочерними div ноде"
   ],
   "metadata": {
    "id": "XuSsrZyfTgka",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "# Beautiful Soup\n",
    "soup = BeautifulSoup(r.text)\n",
    "film_list = soup.find('div', {'class': 'lister-list'})\n",
    "\n",
    "# lxml \n",
    "tree = html.fromstring(r.text)\n",
    "film_list_lxml = tree.xpath('//div[@class = \"lister-list\"]')[0]"
   ],
   "metadata": {
    "id": "gypBdQzcTlYk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "film_list"
   ],
   "metadata": {
    "id": "upX_6r2DToTt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beatiful Soup\n",
    "items = film_list.find_all('h3', {'class': ['lister-item-header']})\n",
    "names_films=[item.find('a').text for item in items]\n",
    "\n",
    "\n",
    "# lxml\n",
    "items_lxml = film_list_lxml.xpath('//h3[@class = \"lister-item-header\"]')\n",
    "names_films_lxml = [item_lxml.xpath('.//a/text()')[0] for item_lxml in items_lxml]\n",
    "\n",
    "names_films_lxml"
   ],
   "metadata": {
    "id": "KuwoLcOoTvi9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beatiful Soup\n",
    "items = film_list.find_all('div', {'class': ['ratings-bar']})\n",
    "strong_films=[item.find('div', {'class':'inline-block ratings-imdb-rating'}).find('strong').text for item in items]\n",
    "\n",
    "\n",
    "# lxml\n",
    "items_lxml = film_list_lxml.xpath('//div[@class = \"ratings-bar\"]')\n",
    "strong_films_lxml = [item_lxml.xpath('.//div[@class = \"inline-block ratings-imdb-rating\"]/strong/text()')[0] for item_lxml in items_lxml]\n",
    "\n",
    "strong_films_lxml"
   ],
   "metadata": {
    "id": "iRONJrSjTzV-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dictionary_films = dict(zip(names_films, strong_films))\n",
    "dictionary_films"
   ],
   "metadata": {
    "id": "XGbPIbWvT3UL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Задание 1**\n",
    "1. С сайта https://www.kinopoisk.ru/lists/movies/popular-films/ получить информацию в csv-файл о всех фильмах с помощью библиотек BeatifulSoup и lxml:\n",
    "\n",
    "\n",
    "*   Название фильма\n",
    "*   Год\n",
    "*   продолжительность (мин.)\n",
    "*   страна\n",
    "*   жанр фильма\n",
    "*   актеры\n",
    "*   рейтинг\n",
    "*   режиссёр\n",
    "2. Предусмотреть сбор информации со всех страниц\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "tJteQGAuQ98Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Часть 2. Использование библиотеки selenium"
   ],
   "metadata": {
    "id": "NTTjTuOqfZi7",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# импортим библиотеки\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys"
   ],
   "metadata": {
    "id": "zxhhl6dTf8Bg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Ставим библиотеку selenium - с помощью неё сымитируем нажатие кнопки и прокрутку\n",
    "!pip install selenium\n",
    "!apt-get update # to update ubuntu to correctly run apt install\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ],
   "metadata": {
    "id": "M2pHeffLf_Fn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Запускаем хром\n",
    "wd = webdriver.Chrome('chromedriver',options=chrome_options) \n",
    "wd.get(\"https://ria.ru/location_Arkhangelskaja_oblast/\")\n",
    "wd.implicitly_wait(10)\n",
    "wd.fullscreen_window\n",
    "print(wd.title) # проверяем работу"
   ],
   "metadata": {
    "id": "9irI4cuKgF3r",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def parse():\n",
    "  soup = BeautifulSoup(wd.page_source, features=\"lxml\") # создание объекта BeautifulSoup для работы с DOM моделью. Используем данные с selenium\n",
    "  location_blocks = soup.findAll('div', {'class': 'list-item'}) # получение всех div с классом 'list-item', который содержит всю необходимую информацию\n",
    "  for block in location_blocks:\n",
    "        link = []\n",
    "        title = []\n",
    "        publdate = []\n",
    "        vievCount = []\n",
    "        tags = []\n",
    "        tagsAll = []\n",
    "        data = []\n",
    "\n",
    "        title.append(block.find('div').find('a', {'class': 'list-item__title color-font-hover-only'}).text) # заголовок\n",
    "\n",
    "        link.append(block.find('a', {'class': 'list-item__title color-font-hover-only'}).get('href')) # Ссылка\n",
    "\n",
    "        publdate.append(block.find('div', {'class': 'list-item__info'}).find('div', {'class' : 'list-item__date'}).text) # дата публикации\n",
    "\n",
    "        vievCount.append(block.find('div', {'class': 'list-item__info'}).find('div', {'class' : 'list-item__views'}).find('div', {'class' : 'list-item__views-text'}).text) # количество просмотров\n",
    "      \n",
    "        tagsAll = block.find('ul') # берем весь список тегов\n",
    "        for itemLi in tagsAll:\n",
    "          tags.append(itemLi.find('a').text)    # раскидываем каждый элемент\n",
    "        \n",
    "        data.append(title)\n",
    "        data.append(link)\n",
    "        data.append(publdate)\n",
    "        data.append(vievCount)\n",
    "        data.append(tags)\n",
    "        with open('harvest_data.csv',  mode=\"a\", encoding='utf-8-sig') as f: \n",
    "          writer = csv.writer(f)\n",
    "          writer.writerow(data)\n",
    "\n",
    "\n",
    "wait = WebDriverWait(wd, 30)\n",
    "wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'list-more'))) # Ждем пока кнопка \"Показать больше\" будет готова\n",
    "\n",
    "x = 0\n",
    "while x<2000:         # создаем длинный цикл, чтобы проявить статьи вызывающиеся по кнопке \"Показать больше\"\n",
    "  time.sleep(0.5)\n",
    "  wd.execute_script('document.querySelector(\"#content > div > div.layout-rubric__main > div > div.list-more\").click()') # Имитируем нажатие на кнопку исполняя скрипт с указанием click()\n",
    "  x = x+10\n",
    "\n",
    "time.sleep(2) #немного ждем, чтобы не потерять последние статьи\n",
    "parse()"
   ],
   "metadata": {
    "id": "BXl0uQEogJse",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wd.quit()"
   ],
   "metadata": {
    "id": "oFrnFZTggN9w",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Задание 2**\n",
    "\n",
    "Спарсить новости с сата https://www.news29.ru/novosti/ekonomika/ или https://www.pomorie.ru/news/ (на выбор студента) с помощью selenium имитируя нажатие кнопки."
   ],
   "metadata": {
    "id": "Es2vcq2Eg7YJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Часть 3. Краулим сайт с помощью Scrappy"
   ],
   "metadata": {
    "id": "VnuDmg-kjH5v",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Скраулить с сайта http://quotes.toscrape.com информацию о цитатах известных людей"
   ],
   "metadata": {
    "id": "zklJmQBBjYza",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/harrywang/scrapy-tutorial-starter.git\n",
    "!cd scrapy-tutorial-starter"
   ],
   "metadata": {
    "id": "_SrI5nZcjock",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install Scrapy"
   ],
   "metadata": {
    "id": "wVJChsRkjpMo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!scrapy startproject my_first_spider"
   ],
   "metadata": {
    "id": "WmjSr7oZjrYH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!cd my_first_spider"
   ],
   "metadata": {
    "id": "fgTgi6kDjuNz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первая попытка\n",
    "```\n",
    "# # import scrapy\n",
    "# class QuotesSpider(scrapy.Spider):\n",
    "#     name = \"quotes\"\n",
    "# start_urls = ['http://quotes.toscrape.com']\n",
    "# def parse(self, response):\n",
    "#         self.logger.info('hello this is my first spider')\n",
    "#         pass\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "WHKvUf8Sj-3U",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "!scrapy shell http://quotes.toscrape.com/ #запускаем Паука\n",
    "\n",
    "Пробуем в командной строке получать информацию\n",
    "\n",
    "\n",
    "```\n",
    "quotes = response.xpath(\"//div[@class='quote']\")\n",
    "quotes[0].css(\".text::text\").getall()\n",
    "quotes[0].css(\".author::text\").getall()\n",
    "quotes[0].css(\".tag::text\").getall()\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "LVhcEyBSkGU0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!scrapy shell http://quotes.toscrape.com/ #запускаем Паука"
   ],
   "metadata": {
    "id": "_Hu8y2VykPxS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь напишем паука\n",
    "\n",
    "\n",
    "```\n",
    "# import scrapy\n",
    "\n",
    "# class QuotesSpider(scrapy.Spider):\n",
    "#   name = \"quotes\"\n",
    "#   start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "#   def parse(self, response):\n",
    "#     self.logger.info('hello this is my first spider')\n",
    "#     quotes = response.css('div.quote')\n",
    "#     for quote in quotes:\n",
    "#         yield {\n",
    "#             'text': quote.css('.text::text').get(),\n",
    "#             'author': quote.css('.author::text').get(),\n",
    "#             'tags': quote.css('.tag::text').getall(),\n",
    "#         }\n",
    "#     next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "#     cnt = 0\n",
    "#     if next_page is not None or cnt<10:\n",
    "#       cnt+=1\n",
    "#       yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "f9m8MrBYkWVC",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!scrapy runspider /content/my_first_spider/my_first_spider/spiders/quotes-spider.py -o quotes.json\n",
    "\n",
    " #запускаем обновленного Паука"
   ],
   "metadata": {
    "id": "RdDQw_mlkSwX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!less quotes.json"
   ],
   "metadata": {
    "id": "3H2WmLJkkia2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Задание 3**\n",
    "\n",
    "Создайте паука для краулинга информации с бесконечно прокручивающийся страницы (http://spidyquotes.herokuapp.com/scroll)"
   ],
   "metadata": {
    "id": "9tl-eG-ekoEc",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}